{"ast":null,"code":"import _slicedToArray from \"C:\\\\PythonProjects\\\\Fashion\\\\YOLOTFjf\\\\tfjs-yolov5-example\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/slicedToArray\";\n\n/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, env, upcastType } from '@tensorflow/tfjs-core';\nimport { BinaryOpProgram } from '../binaryop_gpu';\nimport { BinaryOpPackedProgram } from '../binaryop_packed_gpu';\nimport { complex } from '../kernels/Complex';\nimport { LEAKYRELU, LEAKYRELU_PACKED } from '../kernels/LeakyRelu';\nimport { PRELU, PRELU_PACKED } from '../kernels/Prelu';\nimport * as unary_op from '../unaryop_gpu';\nimport { UnaryOpProgram } from '../unaryop_gpu';\nimport * as unary_packed_op from '../unaryop_packed_gpu';\nimport { UnaryOpPackedProgram } from '../unaryop_packed_gpu';\nexport var CHECK_NAN_SNIPPET_UNARY = \"if (isnan(x)) return x;\";\nexport var CHECK_NAN_SNIPPET_BINARY = \"\\n  if (isnan(a)) return a;\\n  if (isnan(b)) return b;\\n\";\nexport var CHECK_NAN_SNIPPET_BINARY_PACKED = \"\\n  result.r = isNaN.r > 0. ? NAN : result.r;\\n  result.g = isNaN.g > 0. ? NAN : result.g;\\n  result.b = isNaN.b > 0. ? NAN : result.b;\\n  result.a = isNaN.a > 0. ? NAN : result.a;\\n\";\n/**\n * Template that creates a `KernelFunc` for unary ops.\n * @param opSnippet Op snippet to create `UnaryOpProgram`.\n * @param packedOpSnippet Op snippet to create `UnaryOpPackedProgram`.\n * @param dtype Optional. If set, the result has this dtype. Otherwise, the\n *     result has the same dtype as the first input. This is mainly used in\n *     comparison kernels, such as Equal, Less, Greater, etc.\n */\n\nexport function unaryKernelFunc(_ref) {\n  var opSnippet = _ref.opSnippet,\n      packedOpSnippet = _ref.packedOpSnippet,\n      cpuKernelImpl = _ref.cpuKernelImpl,\n      dtype = _ref.dtype;\n  return function (_ref2) {\n    var inputs = _ref2.inputs,\n        backend = _ref2.backend;\n    var x = inputs.x;\n    var webglBackend = backend;\n    var $dtype = dtype || x.dtype;\n\n    if (webglBackend.shouldExecuteOnCPU([x]) && cpuKernelImpl != null) {\n      var xData = webglBackend.texData.get(x.dataId);\n      var outValues = cpuKernelImpl(xData.values, $dtype);\n      return webglBackend.makeTensorInfo(x.shape, $dtype, outValues);\n    }\n\n    var shouldUsePackedProgram = env().getBool('WEBGL_PACK_UNARY_OPERATIONS') && packedOpSnippet != null;\n    var program;\n\n    if (shouldUsePackedProgram) {\n      program = new UnaryOpPackedProgram(x.shape, packedOpSnippet);\n    } else {\n      program = new UnaryOpProgram(x.shape, opSnippet);\n    }\n\n    return webglBackend.runWebGLProgram(program, [x], $dtype);\n  };\n}\n/**\n * Template that creates a `KernelFunc` for binary ops.\n * @param opSnippet Op snippet to create `BinaryOpProgram`.\n * @param packedOpSnippet Op snippet to create `BinaryOpPackedProgram`.\n * @param checkOutOfBoundsForPackedProgram Whether to set checkOutOfBounds=true\n *     when creating BinaryOpPackedProgram.\n * @param dtype Optional. If set, the result has this dtype. Otherwise, the\n *     result has the same dtype as the first input. This is mainly used in\n *     comparison kernels, such as Equal, Less, Greater, etc.\n */\n\nexport function binaryKernelFunc(_ref3) {\n  var opSnippet = _ref3.opSnippet,\n      packedOpSnippet = _ref3.packedOpSnippet,\n      _ref3$checkOutOfBound = _ref3.checkOutOfBounds,\n      checkOutOfBounds = _ref3$checkOutOfBound === void 0 ? false : _ref3$checkOutOfBound,\n      _ref3$supportsComplex = _ref3.supportsComplex,\n      supportsComplex = _ref3$supportsComplex === void 0 ? false : _ref3$supportsComplex,\n      cpuKernelImpl = _ref3.cpuKernelImpl,\n      dtype = _ref3.dtype;\n  return function (_ref4) {\n    var inputs = _ref4.inputs,\n        backend = _ref4.backend;\n    var a = inputs.a,\n        b = inputs.b;\n    var webglBackend = backend;\n\n    if (supportsComplex && a.dtype === 'complex64') {\n      var aData = webglBackend.texData.get(a.dataId);\n      var bData = webglBackend.texData.get(b.dataId);\n\n      var _map = [[aData.complexTensorInfos.real, bData.complexTensorInfos.real], [aData.complexTensorInfos.imag, bData.complexTensorInfos.imag]].map(function (complexParts) {\n        var _complexParts = _slicedToArray(complexParts, 2),\n            aPart = _complexParts[0],\n            bPart = _complexParts[1];\n\n        var aHandle = {\n          dataId: aPart.dataId,\n          dtype: aPart.dtype,\n          shape: a.shape\n        };\n        var bHandle = {\n          dataId: bPart.dataId,\n          dtype: bPart.dtype,\n          shape: b.shape\n        };\n        var program = new BinaryOpProgram(opSnippet, a.shape, b.shape);\n        return webglBackend.runWebGLProgram(program, [aHandle, bHandle], upcastType(aPart.dtype, bPart.dtype));\n      }),\n          _map2 = _slicedToArray(_map, 2),\n          real = _map2[0],\n          imag = _map2[1];\n\n      var complexOutput = complex({\n        inputs: {\n          real: real,\n          imag: imag\n        },\n        backend: webglBackend\n      });\n      webglBackend.disposeIntermediateTensorInfo(real);\n      webglBackend.disposeIntermediateTensorInfo(imag); // TODO(annxingyuan): Implement CPU forwarding for complex inputs.\n\n      return complexOutput;\n    }\n\n    var $dtype = dtype || upcastType(a.dtype, b.dtype);\n\n    if ((a.dtype === 'string' || b.dtype === 'string' || webglBackend.shouldExecuteOnCPU([a, b])) && cpuKernelImpl != null) {\n      var aVals = webglBackend.texData.get(a.dataId).values;\n      var bVals = webglBackend.texData.get(b.dataId).values;\n      var decodedAVals = a.dtype === 'string' ? // tslint:disable-next-line: no-any\n      backend_util.fromUint8ToStringArray(aVals) : aVals;\n      var decodedBVals = a.dtype === 'string' ? // tslint:disable-next-line: no-any\n      backend_util.fromUint8ToStringArray(bVals) : bVals;\n\n      var _cpuKernelImpl = cpuKernelImpl(a.shape, b.shape, decodedAVals, decodedBVals, $dtype),\n          _cpuKernelImpl2 = _slicedToArray(_cpuKernelImpl, 2),\n          outValues = _cpuKernelImpl2[0],\n          outShape = _cpuKernelImpl2[1];\n\n      var out = webglBackend.makeTensorInfo(outShape, $dtype);\n      var outData = webglBackend.texData.get(out.dataId);\n      outData.values = outValues;\n      return out;\n    }\n\n    var shouldUsePackedProgram = env().getBool('WEBGL_PACK_BINARY_OPERATIONS') && packedOpSnippet != null;\n    var program;\n\n    if (shouldUsePackedProgram) {\n      program = new BinaryOpPackedProgram(packedOpSnippet, a.shape, b.shape, checkOutOfBounds);\n    } else {\n      program = new BinaryOpProgram(opSnippet, a.shape, b.shape);\n    }\n\n    return webglBackend.runWebGLProgram(program, [a, b], $dtype);\n  };\n}\nexport function mapActivationToShaderProgram(activation) {\n  var packed = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n\n  if (activation === 'linear') {\n    if (packed) {\n      return unary_packed_op.LINEAR;\n    }\n\n    return unary_op.LINEAR;\n  } else if (activation === 'relu') {\n    if (packed) {\n      return unary_packed_op.RELU;\n    }\n\n    return unary_op.RELU;\n  } else if (activation === 'elu') {\n    if (packed) {\n      return unary_packed_op.ELU;\n    }\n\n    return unary_op.ELU;\n  } else if (activation === 'relu6') {\n    if (packed) {\n      return unary_packed_op.RELU6;\n    }\n\n    return unary_op.RELU6;\n  } else if (activation === 'prelu') {\n    if (packed) {\n      return PRELU_PACKED;\n    }\n\n    return PRELU;\n  } else if (activation === 'leakyrelu') {\n    if (packed) {\n      return LEAKYRELU_PACKED;\n    }\n\n    return LEAKYRELU;\n  } else if (activation === 'sigmoid') {\n    if (packed) {\n      return unary_packed_op.SIGMOID;\n    }\n\n    return unary_op.SIGMOID;\n  }\n\n  throw new Error(\"Activation \".concat(activation, \" has not been implemented for the WebGL backend.\"));\n}","map":null,"metadata":{},"sourceType":"module"}