{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape'; // For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\n\nexport function conv2dByMatMul(_ref) {\n  var x = _ref.x,\n      filter = _ref.filter,\n      convInfo = _ref.convInfo,\n      backend = _ref.backend,\n      _ref$bias = _ref.bias,\n      bias = _ref$bias === void 0 ? null : _ref$bias,\n      _ref$preluActivationW = _ref.preluActivationWeights,\n      preluActivationWeights = _ref$preluActivationW === void 0 ? null : _ref$preluActivationW,\n      _ref$leakyreluAlpha = _ref.leakyreluAlpha,\n      leakyreluAlpha = _ref$leakyreluAlpha === void 0 ? 0 : _ref$leakyreluAlpha,\n      _ref$activation = _ref.activation,\n      activation = _ref$activation === void 0 ? null : _ref$activation;\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  var xShape = x.shape;\n  var xTexData = backend.texData.get(x.dataId);\n  var sharedMatMulDim = convInfo.inChannels;\n  var outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  var outerShapeFilter = convInfo.outChannels;\n  var isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  var transposeA = false;\n  var transposeB = false;\n  var out;\n  var intermediates = []; // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n\n  var batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD; // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n\n  var canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked && isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 && util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    var targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    var xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    var originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), function () {\n      return \"packed reshape \".concat(xTexData.shape, \" to \").concat(xReshaped.shape, \" isn't free\");\n    });\n    var filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend: backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    var pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend: backend,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n    var pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, function () {\n      return 'batchMatMul result is expected to be packed';\n    }); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend: backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  } else {\n    var _targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n\n    var _xReshaped = reshape({\n      inputs: {\n        x: x\n      },\n      backend: backend,\n      attrs: {\n        shape: [1, _targetShape, convInfo.inChannels]\n      }\n    });\n\n    var _filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend: backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n\n    var result = batchMatMulImpl({\n      a: _xReshaped,\n      b: _filterReshaped,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      backend: backend,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend: backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(_xReshaped);\n    intermediates.push(_filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (var _i = 0, _intermediates = intermediates; _i < _intermediates.length; _i++) {\n    var i = _intermediates[_i];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n} // Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\n\nexport function conv2dWithIm2Row(_ref2) {\n  var x = _ref2.x,\n      filter = _ref2.filter,\n      convInfo = _ref2.convInfo,\n      backend = _ref2.backend,\n      _ref2$bias = _ref2.bias,\n      bias = _ref2$bias === void 0 ? null : _ref2$bias,\n      _ref2$preluActivation = _ref2.preluActivationWeights,\n      preluActivationWeights = _ref2$preluActivation === void 0 ? null : _ref2$preluActivation,\n      _ref2$leakyreluAlpha = _ref2.leakyreluAlpha,\n      leakyreluAlpha = _ref2$leakyreluAlpha === void 0 ? 0 : _ref2$leakyreluAlpha,\n      _ref2$activation = _ref2.activation,\n      activation = _ref2$activation === void 0 ? null : _ref2$activation;\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  var filterWidth = convInfo.filterWidth,\n      filterHeight = convInfo.filterHeight,\n      inChannels = convInfo.inChannels,\n      outWidth = convInfo.outWidth,\n      outHeight = convInfo.outHeight,\n      dataFormat = convInfo.dataFormat;\n  var isChannelsLast = dataFormat === 'channelsLast';\n  var sharedDim = filterWidth * filterHeight * inChannels;\n  var numCols = outHeight * outWidth;\n  var x2ColShape = [sharedDim, numCols];\n  var transposeA = true;\n  var transposeB = false;\n  var intermediates = [];\n  var xSqueezed = reshape({\n    inputs: {\n      x: x\n    },\n    backend: backend,\n    attrs: {\n      shape: x.shape.slice(1)\n    }\n  });\n  var w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend: backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n  var im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  var customValues = [xSqueezed.shape, [convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels], [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]];\n  var im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32', customValues);\n  var im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend: backend,\n    attrs: {\n      shape: [1, x2ColShape[0], x2ColShape[1]]\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  var hasBias = bias != null;\n  var hasPreluActivationWeights = preluActivationWeights != null;\n  var hasLeakyreluAlpha = activation === 'leakyrelu';\n  var fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  var matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  var inputs = [im2ColReshaped, w2Row];\n\n  if (bias) {\n    inputs.push(bias);\n  }\n\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n\n  if (hasLeakyreluAlpha) {\n    var $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n\n  var product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  var outShape = isChannelsLast ? [1, outHeight, outWidth, convInfo.outChannels] : [1, convInfo.outChannels, outHeight, outWidth];\n  var out = reshape({\n    inputs: {\n      x: product\n    },\n    backend: backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(product);\n\n  for (var _i2 = 0, _intermediates2 = intermediates; _i2 < _intermediates2.length; _i2++) {\n    var i = _intermediates2[_i2];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}","map":null,"metadata":{},"sourceType":"module"}