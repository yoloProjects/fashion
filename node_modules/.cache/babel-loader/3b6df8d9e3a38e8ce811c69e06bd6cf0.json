{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Softmax, util } from '@tensorflow/tfjs-core';\nimport { exp } from './Exp';\nimport { max } from './Max';\nimport { div } from './RealDiv';\nimport { reshape } from './Reshape';\nimport { sub } from './Sub';\nimport { sum } from './Sum';\nexport function softmax(args) {\n  var inputs = args.inputs,\n      backend = args.backend,\n      attrs = args.attrs;\n  var logits = inputs.logits;\n  var dim = attrs.dim;\n  var logitsRank = logits.shape.length;\n  var $dim = dim;\n\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n\n  if ($dim !== logitsRank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + \"Logits was rank \".concat(logitsRank, \" and dim was \").concat($dim));\n  }\n\n  var axes = util.parseAxisParam([$dim], logits.shape);\n  var maxLogit = max({\n    inputs: {\n      x: logits\n    },\n    backend: backend,\n    attrs: {\n      reductionIndices: axes,\n      keepDims: false\n    }\n  });\n  var expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n  var maxLogitReshaped = reshape({\n    inputs: {\n      x: maxLogit\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var a = sub({\n    inputs: {\n      a: logits,\n      b: maxLogitReshaped\n    },\n    backend: backend\n  });\n  var b = exp({\n    inputs: {\n      x: a\n    },\n    backend: backend\n  });\n  var sumExp = sum({\n    inputs: {\n      x: b\n    },\n    backend: backend,\n    attrs: {\n      axis: axes,\n      keepDims: false\n    }\n  });\n  var sumReshaped = reshape({\n    inputs: {\n      x: sumExp\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var result = div({\n    inputs: {\n      a: b,\n      b: sumReshaped\n    },\n    backend: backend\n  });\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n  return result;\n}\nexport var softmaxConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax\n};","map":null,"metadata":{},"sourceType":"module"}